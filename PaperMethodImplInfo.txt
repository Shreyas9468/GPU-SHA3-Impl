Overview: What’s the Paper Doing?
The paper optimizes SHA-3 (Keccak-f[1600]) for GPUs:

θ: PTX inline assembly (Algorithm 2)—faster XOR operations.
ρ+π: Direct indexing—no π table (Section III.D)—cuts lookup overhead.
Coalesced Memory: Column-wise input (Section III.E)—threads read/write aligned 64-bit words—GPU loves this.
Single Stream: 88.51 Gb/s baseline—parallelizes 1M+ messages (~2.41 GB).
Our code hashes 1,048,576 messages (~2.41 GB) on your RTX 4060—each thread processes one message—mirroring the paper’s approach.

1. generate_input—Preparing Input Data : 

void generate_input(uint64_t *input, size_t num_messages, size_t num_blocks) {
    srand(time(NULL));
    for (size_t i = 0; i < num_blocks; i++) {
        for (size_t m = 0; m < num_messages; m++) {
            uint64_t *block = input + i * num_messages * BLOCK_WORDS + m * BLOCK_WORDS;
            for (size_t j = 0; j < BLOCK_WORDS - 1; j++) {
                block[j] = ((uint64_t)rand() << 32) | rand();
            }
            block[BLOCK_WORDS - 1] = (i == num_blocks - 1) ? 0x8000000000000006ULL : 0;
        }
    }
}

What: Fills input (~2.41 GB) with random data—1M messages, 32 blocks each, 72 bytes per block—runs on CPU.
Indexing:
    3D Structure: input—flat array—imagined as blocks (32) × messages (1M) × words (9)—~33.5M 64-bit words.

    Formula: block = input + i * num_messages * BLOCK_WORDS + m * BLOCK_WORDS
        i—block index (0-31)—num_messages * BLOCK_WORDS—skips 1M messages’ worth of words (~9.4M words).
        m—message index (0-1,048,575)—BLOCK_WORDS = 9—skips 9 words per message.
        E.g., message 100, block 2: 2 * 1048576 * 9 + 100 * 9 = 18,874,368 + 900 = 18,875,268—points to 18,875,268th word.
    
    Column-Wise: i * num_messages * BLOCK_WORDS—block 0 for all messages, then block 1—threads read contiguous 64-bit words—paper’s coalesced memory (Section III.E).

Methods:
    Random Fill: block[j] = ((uint64_t)rand() << 32) | rand()—8 words (~64 bytes)—64-bit random numbers—mimics real data—paper assumes input.
    Padding: block[8] = (i == 31) ? 0x8000000000000006ULL : 0—last block gets SHA-3 padding (0x06 + 0x80)—others zero—standard multi-block SHA-3—paper’s approach.

Why: Prepares ~2.41 GB—1M threads process in parallel—coalesced indexing speeds GPU reads—padding ensures SHA-3 correctness.

2. keccak_f—SHA-3’s Core Function : 

    __device__ void keccak_f(uint64_t *state) {
        const int rho_offsets[25] = {0, 1, 62, 28, 27, 36, 44, 6, 55, 20, 3, 10, 43, 25, 39, 41, 45, 15, 21, 8, 18, 2, 61, 56, 14};
        for (int round = 0; round < ROUNDS; round++) {
            uint64_t C[5], D[5];
            #pragma unroll
            for (int x = 0; x < 5; x++) {
                asm("xor.b64 %0, %1, %2;\n\t"
                    "xor.b64 %0, %0, %3;\n\t"
                    "xor.b64 %0, %0, %4;\n\t"
                    "xor.b64 %0, %0, %5;"
                    : "=l"(C[x])
                    : "l"(state[x]), "l"(state[x + 5]), "l"(state[x + 10]), "l"(state[x + 15]), "l"(state[x + 20]));
            }
            #pragma unroll
            for (int x = 0; x < 5; x++) {
                D[x] = C[(x + 4) % 5] ^ ROTL64(C[(x + 1) % 5], 1);
            }
            #pragma unroll
            for (int i = 0; i < 25; i++) {
                state[i] ^= D[i % 5];
            }

            uint64_t B[25];
            #pragma unroll
            for (int x = 0; x < 5; x++) {
                for (int y = 0; y < 5; y++) {
                    int idx = x + 5 * y;
                    int new_x = y;
                    int new_y = (2 * x + 3 * y) % 5;
                    B[new_x + 5 * new_y] = ROTL64(state[idx], rho_offsets[idx]);
                }
            }

            #pragma unroll
            for (int x = 0; x < 5; x++) {
                for (int y = 0; y < 5; y++) {
                    int idx = x + 5 * y;
                    state[idx] = B[idx] ^ ((~B[(x + 1) % 5 + 5 * y]) & B[(x + 2) % 5 + 5 * y]);
                }
            }
            state[0] ^= d_RC[round];
        }
    }

What: Keccak-f[1600]—SHA-3’s core—transforms a 1600-bit state (25 × 64-bit words)—runs on GPU per thread—called 32 times per message.

Indexing & Methods:
    State: state[25]—1600 bits—5×5 grid—state[i]—linear array—i = x + 5 * y maps 2D to 1D—e.g., state[6] = row 1, col 1.

    θ (Lines 3-11):
        C[x]: XOR 5 words per column—state[x], state[x+5], ...—e.g., C[0] = state[0] ^ state[5] ^ state[10] ^ state[15] ^ state[20].
        PTX: asm("xor.b64 ...")—paper’s optimization (Algorithm 2)—faster than C-style XOR—lowers GPU instructions—key speedup.
        D[x]: Combine C values—D[x] = C[(x+4)%5] ^ ROTL64(C[(x+1)%5], 1)—e.g., D[0] = C[4] ^ (C[1] << 1)—shifts bits left 1.
        Update State: state[i] ^= D[i % 5]—e.g., state[0] ^= D[0], state[5] ^= D[0]—spreads changes—5 lanes per column.

    ρ+π (Lines 13-19):
        Temp Array B: B[new_x + 5 * new_y] = ROTL64(state[idx], rho_offsets[idx])—rotates and permutes—new_x = y, new_y = (2*x + 3*y) % 5—paper’s direct indexing (Section III.D).
        Indexing: idx = x + 5 * y—original position—e.g., state[6] (x=1, y=1)—new position B[6]—new_x=1, new_y=(2*1+3*1)%5=0—no π table—saves lookups.
        ROTL64: Shifts bits—rho_offsets[6]=44—e.g., state[6] << 44—Keccak’s rotation—optimized in GPU.

    χ (Lines 20-25):
        state[idx] = B[idx] ^ ((~B[(x+1)%5 + 5*y]) & B[(x+2)%5 + 5*y])—non-linear step—e.g., state[0] = B[0] ^ ((~B[1]) & B[2])—bitwise ops per lane.
        Indexing: (x+1)%5 + 5*y—next column—wraps around—GPU-friendly—paper’s raw method.

    ι (Line 26): state[0] ^= d_RC[round]—adds round constant—e.g., state[0] ^= 0x0000000000000001ULL—24 rounds—Keccak’s tweak.
Why:
Runs on GPU—__device__—each thread transforms its state—32 calls per message—SHA-3 sponge—paper’s optimized Keccak-f.

3. sha3_kernel—GPU Parallel Hashing : 

    __global__ void sha3_kernel(const uint64_t *d_input, uint64_t *d_output, size_t num_messages, size_t num_blocks) {
        size_t tid = blockIdx.x * blockDim.x + threadIdx.x;
        if (tid >= num_messages) return;

        uint64_t state[STATE_SIZE] = {0};
        size_t offset = tid * BLOCK_WORDS * num_blocks;

        #pragma unroll
        for (size_t i = 0; i < num_blocks; i++) {
            for (int j = 0; j < BLOCK_WORDS; j++) {
                state[j] ^= d_input[offset + i * BLOCK_WORDS + j];
            }
            keccak_f(state);
        }

        #pragma unroll
        for (int j = 0; j < HASH_WORDS; j++) {
            d_output[tid * HASH_WORDS + j] = state[j];
        }
    }

What: GPU kernel—1M threads hash 1M messages—runs keccak_f 32 times per thread—writes 64-byte hashes.

Indexing & Methods:
Thread ID: tid = blockIdx.x * blockDim.x + threadIdx.x—unique ID—e.g., block 100, thread 50—tid = 100 * 128 + 50 = 12,850—thread 12,850 hashes message 12,850.
    blockIdx.x—block number (0-8191)—blockDim.x = 128—threads per block—threadIdx.x—thread in block (0-127).
    
    Check: if (tid >= num_messages)—8192 × 128 = 1,048,576—exact match—safety for extra threads.

State: state[25]—local 1600-bit array—each thread has own copy—initialized to zero—SHA-3 starts here.

Input Indexing: offset = tid * BLOCK_WORDS * num_blocks—e.g., tid=12,850, 9 * 32 = 288—offset = 12,850 * 288 = 3,700,800—start of message 12,850’s data.
    d_input[offset + i * BLOCK_WORDS + j]—e.g., block 2, word 3—3,700,800 + 2 * 9 + 3 = 3,700,821—coalesced—paper’s method (Section III.E)—adjacent threads read nearby words.

Absorb:
    state[j] ^= d_input[...]—XORs 9 words (~72 bytes) into state—e.g., state[0] ^= d_input[3,700,800]—block data mixed in.

    keccak_f(state)—transforms state—32 calls—processes all blocks—paper’s multi-block approach.

Squeeze: d_output[tid * HASH_WORDS + j] = state[j]—writes 8 words (~64 bytes)—e.g., tid=12,850, 12,850 * 8 = 102,800—d_output[102,800-102,807]—hash output—coalesced write.

Why:
    __global__—GPU kernel—1M threads run in parallel—each hashes one message—paper’s parallel design.

    #pragma unroll—unrolls loops—fewer branches—GPU optimization—paper assumes efficient execution.

4. main—Orchestrating It All : 

    int main() {
        size_t num_messages = 1048576;
        size_t num_blocks = 32;
        size_t input_size = num_messages * num_blocks * BLOCK_WORDS * sizeof(uint64_t);
        size_t output_size = num_messages * HASH_WORDS * sizeof(uint64_t);

        uint64_t *h_input, *h_output;
        cudaMallocHost(&h_input, input_size);
        cudaMallocHost(&h_output, output_size);

        uint64_t *d_input, *d_output;
        cudaMalloc(&d_input, input_size);
        cudaMalloc(&d_output, output_size);

        generate_input(h_input, num_messages, num_blocks);

        int blocks = (num_messages + THREADS_PER_BLOCK - 1) / THREADS_PER_BLOCK;

        cudaEvent_t start, stop;
        cudaEventCreate(&start);
        cudaEventCreate(&stop);

        cudaEventRecord(start, 0);
        cudaMemcpy(d_input, h_input, input_size, cudaMemcpyHostToDevice);
        sha3_kernel<<<blocks, THREADS_PER_BLOCK>>>(d_input, d_output, num_messages, num_blocks);
        cudaMemcpy(h_output, d_output, output_size, cudaMemcpyDeviceToHost);
        cudaEventRecord(stop, 0);

        cudaDeviceSynchronize();

        float total_time_ms;
        cudaEventElapsedTime(&total_time_ms, start, stop);
        double total_time = total_time_ms / 1000.0;
        double throughput = (input_size * 8.0) / (total_time * 1e9);

        printf("Processed %zu messages (%zu bytes each) in %.3f seconds\n",
            num_messages, num_blocks * BLOCK_SIZE_BYTES, total_time);
        printf("Throughput: %.2f Gb/s\n", throughput);
        printf("First hash: ");
        for (int i = 0; i < HASH_WORDS; i++) {
            printf("%016lx ", h_output[i]);
        }
        printf("\n");

        cudaEventDestroy(start);
        cudaEventDestroy(stop);
        cudaFree(d_input);
        cudaFree(d_output);
        cudaFreeHost(h_input);
        cudaFreeHost(h_output);

        return 0;
    }

What: CPU (host) orchestrates—sets up ~2.41 GB, launches GPU kernel, gets ~67 MB hashes, shows throughput.

Indexing & Methods:
    Setup: num_messages = 1048576, num_blocks = 32—1M messages, 32 blocks each—~2.41 GB—paper’s scale.
        input_size—~2.41 GB—output_size—~67 MB—exact sizes for GPU allocation.

    Memory: cudaMallocHost—pinned memory—WSL-friendly—fast H2D (~65-80 Gb/s)—paper assumes efficient transfer (Section IV).
        cudaMalloc—GPU VRAM—~2.48 GB—fits 8 GB RTX 4060—paper’s GPU focus.

    Input: generate_input—fills h_input—coalesced—paper’s method (Section III.E).

    Grid: blocks = (1048576 + 127) / 128 = 8192—8192 blocks × 128 threads—1M tasks—paper’s parallel design.

    Execution:
        cudaMemcpy(...HostToDevice)—H2D—~2.41 GB to GPU—paper’s data prep.
        sha3_kernel<<<8192, 128>>>—launches 1M threads—paper’s core (Section III).
        cudaMemcpy(...DeviceToHost)—D2H—~67 MB back—paper’s result fetch.

    Timing: cudaEvent—measures H2D-to-D2H—paper reports 88.51 Gb/s—our throughput.
    
Why: CPU sets up—GPU hashes in parallel—raw paper flow—throughput added for info.